experiment_name: dpo_v1
seed: 7
backend: accelerate

model:
  policy_model_name_or_path: "checkpoints/sft/latest"
  reference_model_name_or_path: "checkpoints/sft/latest"
  beta: 0.1
  label_smoothing: 0.0

data:
  preference_path: "data/processed/preference_pairs.jsonl"
  num_workers: 4

optimization:
  total_batch_size: 256
  micro_batch_size: 1
  learning_rate: 1.0e-6
  warmup_steps: 100
  max_train_steps: 2000
  lr_scheduler: cosine

logging:
  output_dir: "checkpoints/dpo"
  log_dir: "logs/dpo"
  log_every_steps: 10
  eval_every_steps: 100
  save_every_steps: 200
  use_wandb: true

hardware:
  mixed_precision: bf16
  gradient_accumulation_steps: 256
  num_processes: 8
