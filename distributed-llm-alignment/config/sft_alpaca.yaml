experiment_name: sft_alpaca_hf
seed: 42
backend: accelerate

model:
  model_name_or_path: "mistralai/Mistral-7B-v0.1"
  max_seq_length: 2048
  gradient_checkpointing: true
  use_flash_attention: true

data:
  # pulled from config/data_sources/sft_alpaca.yaml
  source: hf
  hf_path: "yahma/alpaca-cleaned"
  split: "train"
  columns:
    prompt: "instruction"
    response: "output"
  template: "Instruction: {instruction}\n\nInput: {input}\n\nResponse:"
  num_workers: 8
  limit: null

optimization:
  total_batch_size: 512
  micro_batch_size: 4
  learning_rate: 2.0e-5
  warmup_steps: 500
  weight_decay: 0.01
  max_train_steps: 5000
  lr_scheduler: cosine
  max_grad_norm: 1.0

logging:
  output_dir: "checkpoints/sft_alpaca"
  log_dir: "logs/sft_alpaca"
  log_every_steps: 20
  eval_every_steps: 200
  save_every_steps: 500
  use_wandb: true

hardware:
  mixed_precision: bf16
  gradient_accumulation_steps: 32
  num_processes: 8
  deepspeed_config: "config/deepspeed_zero3.json"
