Distributed On-Policy Distillation for Preference-Aligned LLMs
=================================================================
Project Goal
------------
Build a distributed training pipeline for:
1) Supervised fine-tuning (SFT) of an open-source LLM.
2) Post-training alignment via preference learning (DPO or RLHF).
3) On-policy distillation from a larger aligned teacher to a smaller student.
4) Evaluation of alignment, safety, and efficiency.

Assumptions
-----------
- You have access to Linux + GPUs (e.g., Great Lakes or similar cluster).
- You are comfortable with Python, PyTorch, and basic shell scripting.
- You can use `accelerate` or `DeepSpeed` or native PyTorch FSDP for distributed training.

High-Level Phases
-----------------
Phase 0: Project Skeleton & Environment
Phase 1: SFT (Supervised Fine-Tuning) with Distributed Training
Phase 2: Preference Model (Reward Model) Training
Phase 3: Policy Alignment (DPO or RLHF)
Phase 4: On-Policy Distillation (Teacher -> Student)
Phase 5: Evaluation & Ablations
Phase 6: Packaging

============================================================
PHASE 0: PROJECT SKELETON & ENVIRONMENT
============================================================

0.1. Create repo structure
--------------------------
Create a repo (e.g., `distributed-llm-alignment`):

distributed-llm-alignment/
  README.md
  requirements.txt
  scripts/
    launch_sft.sh
    launch_reward.sh
    launch_dpo.sh
    launch_distill.sh
    launch_eval.sh
  config/
    sft_config.yaml
    reward_config.yaml
    dpo_config.yaml
    distill_config.yaml
    eval_config.yaml
  data/
    raw/
    processed/
  src/
    __init__.py
    data/
      __init__.py
      datasets.py
    models/
      __init__.py
      base_model.py
      reward_model.py
    training/
      __init__.py
      train_sft.py
      train_reward.py
      train_dpo.py
      train_rlhf.py   # optional if you do PPO
      train_distill.py
    eval/
      __init__.py
      eval_alignment.py
      eval_latency.py
  logs/
  checkpoints/

0.2. Environment setup
----------------------
- Use Python >= 3.10
- In `requirements.txt` include approximate stack:
  - torch, torchvision, torchaudio (matching CUDA)
  - transformers
  - datasets
  - accelerate
  - deepspeed (optional)
  - sentencepiece
  - bitsandbytes (if you want 4/8-bit)
  - peft (optional)
  - evaluate
  - wandb (optional)
  - tqdm, pyyaml

Example (rough):
torch
transformers
datasets
accelerate
deepspeed
sentencepiece
bitsandbytes
peft
evaluate
wandb
tqdm
pyyaml

- Create and activate venv/conda env.
- Install requirements.

0.3. Cluster / multi-GPU setup
------------------------------
- Decide your distributed backend:
  Option A: `accelerate` + `DeepSpeed` (simpler to script).
  Option B: pure PyTorch FSDP if you want more control.

- Write a basic `scripts/launch_sft.sh` that wraps `accelerate launch` or `torchrun`, e.g.:

  accelerate launch --num_processes 4 --mixed_precision bf16 src/training/train_sft.py --config config/sft_config.yaml

- Verify that a toy script can:
  - Detect all GPUs.
  - Run a tiny forward/backward pass across devices.

============================================================
PHASE 1: SUPERVISED FINE-TUNING (SFT)
============================================================

1.1. Choose base model
----------------------
- Start with a 1–3B parameter model (e.g., open LLaMA/Mistral/phi variant).
- Define in `config/sft_config.yaml`:
  - model_name_or_path
  - max_seq_length
  - global_batch_size, micro_batch_size
  - learning_rate, weight_decay
  - num_train_steps or num_epochs
  - logging + checkpoint paths
  - deepspeed / fsdp settings

1.2. Prepare instruction data
-----------------------------
- Collect or download instruction datasets (e.g., open-source instruction data).
- Normalize into a common JSONL format, e.g.:

  {"prompt": "...", "response": "..."}

- Write `src/data/datasets.py` to load and tokenize:
  - Tokenize prompt+response with EOS.
  - Create labels that mask out prompt tokens if desired.

1.3. Implement `train_sft.py`
-----------------------------
Core responsibilities:
- Parse config.
- Load tokenizer + base model.
- Wrap model with `accelerate` or FSDP/DeepSpeed.
- Build DataLoader from your instruction dataset.
- Standard language modeling loss on response tokens.
- Save checkpoints every N steps and at end of training.

Checks:
- Training loss decreases.
- You can sample from the SFT model locally to inspect quality.

1.4. Outputs to keep
--------------------
- Checkpoints: `checkpoints/sft/step_XX/`
- Logs: `logs/sft/`
- A short script/notebook to generate sample completions from SFT model.

============================================================
PHASE 2: PREFERENCE MODEL (REWARD MODEL)
============================================================

2.1. Prepare preference data
----------------------------
You need (prompt, chosen_response, rejected_response) triples. Options:
- Use existing public preference datasets.
- Or synthesize a small set yourself (can start small).

Store as JSONL:
{"prompt": "...",
 "chosen": "...",
 "rejected": "..."}

2.2. Reward model architecture
------------------------------
- Start from same or smaller base model as the SFT model.
- Add a scalar head on top of the final hidden state (e.g., final token or pooled representation).
- Implementation in `src/models/reward_model.py`.

2.3. Implement `train_reward.py`
--------------------------------
Steps:
- Load base model + reward head.
- For each preference sample:
  - Compute reward r_chosen, r_rejected.
  - Loss = -log(sigmoid(r_chosen - r_rejected)).
- Use distributed training as before (accelerate/DeepSpeed).
- Save best checkpoint by validation loss.

Outputs:
- Reward model checkpoints: `checkpoints/reward/`.
- Plots/logs of training/validation loss over time.

============================================================
PHASE 3: POLICY ALIGNMENT (DPO OR RLHF)
============================================================

3.1. Choose alignment algorithm
-------------------------------
Option A: DPO (Direct Preference Optimization) [simpler].
Option B: RLHF (PPO with reward model) [heavier].
You can start with DPO and optionally extend to PPO later.

3.2. Data for DPO/RLHF
----------------------
- Use the same preference dataset you used for reward model.
- For DPO, you need (prompt, chosen, rejected).
- For RLHF, you’ll also need a sampler that queries the current policy to generate candidate responses.

3.3. Implement `train_dpo.py`
-----------------------------
Core loop:
- Load SFT model as initialization for policy.
- For each batch:
  - Tokenize prompt + chosen, prompt + rejected.
  - Compute log probabilities under current policy and reference policy (usually SFT model).
  - DPO loss uses preference pairs and a temperature / beta parameter.
- Distributed training as before.

3.4. (Optional) Implement `train_rlhf.py` (PPO)
-----------------------------------------------
- Use SFT model as initial policy, and SFT or base model as reference.
- Sample responses from policy on prompts (on-policy).
- Compute rewards via reward model + KL penalty.
- Do PPO updates.

3.5. Outputs to keep
--------------------
- Checkpoints: `checkpoints/dpo/` or `checkpoints/rlhf/`.
- Logs of reward / preference metrics.
- A sampling script for the aligned teacher model.

============================================================
PHASE 4: ON-POLICY DISTILLATION (TEACHER -> STUDENT)
============================================================

4.1. Define teacher and student
-------------------------------
- Teacher: aligned policy from Phase 3 (DPO/RLHF checkpoint).
- Student: smaller model (e.g., half or quarter of parameters).

4.2. Collect on-policy data from teacher
----------------------------------------
- Build `src/training/generate_teacher_data.py`:
  - Load teacher.
  - Sample responses for a large set of prompts (instruction + safety prompts).
  - Save JSONL:
    {"prompt": "...", "teacher_response": "..."}

- Optionally store teacher logits if feasible (larger disk cost).

4.3. Implement `train_distill.py`
---------------------------------
- Load student model.
- For each example:
  - Tokenize prompt + teacher_response.
  - Train student to match teacher distribution (KL / cross-entropy).
- Use distributed training for scale.
- Optionally weight examples by reward model score (upweight high-reward trajectories).

4.4. Outputs
------------
- Student checkpoints: `checkpoints/distill/`.
- Logs of training loss.
- Comparison of inference speed (student vs teacher).

============================================================
PHASE 5: EVALUATION & ABLATIONS
============================================================

5.1. Define evaluation benchmarks
---------------------------------
At minimum:
- General instruction-following: held-out instruction set.
- Safety / harmlessness: prompts for disallowed content.
- Truthfulness / hallucination: factual QA set.

5.2. Implement `eval_alignment.py`
----------------------------------
- For each model variant:
  - Base model.
  - SFT model.
  - DPO/RLHF teacher.
  - Distilled student.
- Run:
  - Automatic metrics (reward model score, toxicity classifier if you use one, refusal rate).
  - Basic text statistics: length, diversity, etc.

Output:
- Tables/JSON with metrics per model variant.
- At least one result table you can paste into your resume/portfolio.

5.3. Implement `eval_latency.py`
--------------------------------
- Measure per-token / per-sequence latency on same hardware.
- Compare teacher vs student:
  - Tokens/sec.
  - VRAM usage.
  
Keep a concise summary like:
- “Student model: 3.8× faster, 0.7× VRAM, retains 93% of alignment score vs teacher.”

5.4. Ablation ideas
-------------------
- SFT vs DPO vs RLHF on safety metrics.
- Off-policy distillation (generic instruction data) vs on-policy distillation (teacher trajectories).
- Impact of reward-weighted distillation vs unweighted.

============================================================
PHASE 6: PACKAGING FOR RESUME / PORTFOLIO
============================================================

6.1. Polish README.md
---------------------
Include:
- Short abstract.
- Diagram of pipeline: SFT -> Reward -> DPO/RLHF -> Distill -> Eval.
- Instructions to run each phase (copy-paste commands).
- Example evaluation table.
